# ğŸ—ƒ RAG - PPC de Licenciatura em ComputaÃ§Ã£o (UFRPE)

Este projeto implementa uma soluÃ§Ã£o **Retrieval-Augmented Generation** para responder perguntas com base no Projeto PedagÃ³gico do Curso de Licenciatura em ComputaÃ§Ã£o da UFRPE. O objetivo Ã© fornecer respostas eficientes e precisas, combinando a recuperaÃ§Ã£o de informaÃ§Ãµes de um documento especÃ­fico com a capacidade de geraÃ§Ã£o de um modelo de linguagem.
---

## ğŸ“š Base de Conhecimento Utilizada

O sistema RAG utiliza o seguinte documento como sua base de conhecimento:

* [**PPC - Licenciatura em ComputaÃ§Ã£o**](http://www.lc.ufrpe.br/sites/ww2.lc.ufrpe.br/files/PPC-LC-atualizao.pdf)

---

## âœ¨ Funcionalidades

- **ğŸ“„ ExtraÃ§Ã£o de Texto**  
   Leitura e prÃ©-processamento do conteÃºdo de um arquivo PDF.

-  **âœ‚ï¸ Chunking e Processamento**  
   SegmentaÃ§Ã£o do texto em parÃ¡grafos/chunks, com sobreposiÃ§Ã£o para preservar o contexto semÃ¢ntico.

- **ğŸ” RecuperaÃ§Ã£o de InformaÃ§Ã£o com BM25**  
   IndexaÃ§Ã£o dos trechos utilizando o algoritmo BM25 para busca textual eficiente e relevante.

- **ğŸ§  GeraÃ§Ã£o de Respostas com LLMs**  
   Uso de modelos de linguagem baseados em transformadores  para gerar respostas a partir dos trechos recuperados.

-  **ğŸ’¬ Interface Interativa**  
   Interface de uso simples e acessÃ­vel, com versÃµes implementadas em:
   - `Streamlit` (para execuÃ§Ã£o local)
   - `Gradio` (para deploy rÃ¡pido e compartilhÃ¡vel)

---

## ğŸ§± Arquitetura
Nos baseamos em arquiteturas para construir o projeto, utilizando os dois fluxos abaixo como referenciais para nossas decisÃµes.

  ![RAG PadrÃ£o](https://i.imgur.com/RjdXPvf.png)
---
![RAG BM25](https://i.imgur.com/ouTP7HW.png)

- A partir dessas ideias, implementamos um fluxo adaptado em que o documento PDF Ã© carregado, seu conteÃºdo Ã© extraÃ­do e segmentado em chunks textuais com sobreposiÃ§Ã£o. 
- Diferente das abordagens com embeddings e bancos vetoriais, seguimos o caminho do TF-IDF para indexaÃ§Ã£o lexical, e aplicamos o algoritmo BM25 para ranquear os trechos mais relevantes com base na consulta do usuÃ¡rio. 
- Essa estratÃ©gia permite recuperar os chunks mais prÃ³ximos da pergunta de forma eficiente, sem necessidade de modelos de embeddings, mantendo a lÃ³gica geral do RAG e combinando o prÃ©-processamento textual com modelos para resposta.

## ğŸš€ Tecnologias e Bibliotecas

Durante o desenvolvimento do projeto, foram aplicadas as seguintes tecnologias e bibliotecas Python:

* **Python**: Linguagem de programaÃ§Ã£o principal.
* **Streamlit**: Framework para criaÃ§Ã£o da interface web interativa do chatbot.
* **Gradio**: Biblioteca que permite a criaÃ§Ã£o rÃ¡pida e fÃ¡cil de interfaces web interativas para deploy de modelos.
* **PyMuPDF (fitz)**: Utilizado para a extraÃ§Ã£o eficiente de texto de arquivos PDF.
* **NLTK**: Kit de ferramentas de Processamento de Linguagem Natural, empregado para tokenizaÃ§Ã£o de texto.
* **Rank-BM25**: ImplementaÃ§Ã£o do algoritmo BM25 para a fase de recuperaÃ§Ã£o de documentos (ranking de relevÃ¢ncia).
* **Hugging Face Transformers**: Biblioteca para acesso e utilizaÃ§Ã£o de modelos de linguagem prÃ©-treinados.

---

## ğŸ“¦ Estrutura do Projeto

A organizaÃ§Ã£o do projeto segue uma estrutura modular para facilitar o desenvolvimento, a manutenÃ§Ã£o e a colaboraÃ§Ã£o:

```bash
ğŸ“ data/
â”‚   â””â”€â”€ PPC-LC-atualizao.pdf           # Documento-base (PPC)
ğŸ“ notebook/
â”‚   â”œâ”€â”€ [GRADIO]_PPC_t5_large.ipynb    # DemonstraÃ§Ã£o com Gradio + Modelo T5-Large
â”‚   â”œâ”€â”€ colab.ipynb                    # CÃ³digo adaptado para Google Colab
â”‚   â”œâ”€â”€ extracao_ementas_obrigatorias.ipynb # Exemplo que foi feito usando o Ementas ObrigatÃ³rias, mas que foi descontinuado por dificuldades.
â”‚   â””â”€â”€ ppc_extracao_indexacao.ipynb   # ExtraÃ§Ã£o + BM25
ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py                     # FunÃ§Ãµes de extraÃ§Ã£o de texto
â”‚   â”œâ”€â”€ preprocessing.py               # Limpeza e chunking
â”‚   â””â”€â”€ rag.py                         # BM25 + integraÃ§Ã£o com modelo
ğŸ“ streamlit_app/
â”‚   â””â”€â”€ rag_interface.py               # Interface com Streamlit + Modelo T5-Base de pierreguillou
â”œâ”€â”€ app.py                             # ExecuÃ§Ã£o principal (opcional)
â”œâ”€â”€ requirements.txt                   # Lista de dependÃªncias
â””â”€â”€ README.md                          # Este arquivo
```
## âš  Dificuldades e LimitaÃ§Ãµes

Durante o desenvolvimento do projeto, enfrentamos algumas dificuldades e encontramos diversas limitaÃ§Ãµes.

A principal delas foi a escolha e comportamento dos modelos de linguagem. Testamos diferentes modelos, e nem todos conseguiam trazer as respostas certas com precisÃ£o, ou sequer mesmo alguma resposta cabÃ­vel. Muitas vezes, o tempo de processamento era alto para obter uma resposta mais completa e contextualizada, e ainda assim o modelo podia apresentar alucinaÃ§Ãµes â€” inserindo informaÃ§Ãµes erradas ou incompletas, mesmo quando dÃ¡vamos palavras-chave especÃ­ficas. 

TambÃ©m tivemos dificuldade com o deploy das interfaces. O Streamlit funcionou localmente sem muitos problemas, mas o Gradio, sempre que subÃ­amos no Google Colab, apresentava erro ao tentar carregar o notebook jÃ¡ rodado. A soluÃ§Ã£o foi deixar o notebook do Gradio salvo sem execuÃ§Ã£o, apenas com uma imagem de referÃªncia para quando fosse baixado e testado. Para testar a aplicaÃ§Ã£o, Ã© necessÃ¡rio abrir o notebook manualmente no Colab, inserir o arquivo do PPC no ambiente, executar as cÃ©lulas e acessar o link gerado.

Por fim, outro desafio foi ajustar o comportamento do modelo para tentar gerar respostas conversativas. O foco era que fosse recuperado trechos relevantes do documento de forma direta e correta, mas nem sempre foi possÃ­vel, e em segundo plano, tentamos embelezar a contextualizaÃ§Ã£o, para trazer respostas seguidas de uma contextualizaÃ§Ã£o e em tom de conversa. Em vÃ¡rios casos, a resposta era apenas uma repetiÃ§Ã£o do trecho recuperado, sem o nÃ­vel de clareza ou elaboraÃ§Ã£o que buscÃ¡vamos, como vemos em ferramentas mais robustas como o ChatGPT, DeepSeek, Gemini etc.
