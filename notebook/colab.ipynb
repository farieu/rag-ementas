# Análise Exploratória do PPC
# Este notebook mostra como o texto do PPC é extraído, tokenizado e indexado para uso no modelo RAG.

# --- Configuração Inicial e Instalações (se necessário no Colab) ---
# Se você estiver rodando em um ambiente Colab novo, pode precisar instalar as libs
# !pip install PyMuPDF nltk rank-bm25 transformers python-dotenv google-generativeai # ou openai

import os
from dotenv import load_dotenv

# Carrega as variáveis de ambiente (se você for testar a API Gemini/OpenAI aqui)
load_dotenv()

# --- Downloads do NLTK ---
# É importante rodar esses downloads no notebook também para garantir que os recursos estejam presentes.
import nltk
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except nltk.downloader.DownloadError:
    nltk.download('punkt_tab')

# --- Importações das Funções do app.py ---
# Se 'analise_inicial.ipynb' está em 'notebook/' e 'app.py' está na raiz,
# precisamos adicionar a raiz do projeto ao sys.path.

import sys
# O 'os.path.abspath(os.path.join(os.getcwd(), '..'))' leva ao diretório pai,
# que agora é a raiz do seu projeto se você estiver em 'notebooks/'.
# Isso garante que 'app.py' seja encontrado.
project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))
if project_root not in sys.path:
    sys.path.append(project_root)

# Agora o import é direto, pois 'app.py' está na raiz e a raiz foi adicionada ao sys.path
from app import extrair_texto_pdf, preparar_paragrafos, tokenizar_paragrafos, configurar_bm25
from nltk.tokenize import word_tokenize # Precisa importar word_tokenize aqui também

# --- Extração do texto ---
print("# Extração do texto do PDF")
# O caminho '../data/PPC-LC-atualizao.pdf' está correto se o notebook está em 'notebooks/'
# e o 'data/' está na raiz do projeto.
texto = extrair_texto_pdf("../data/PPC-LC-atualizao.pdf")
print(f"Primeiros 1000 caracteres do texto extraído:\n{texto[:1000]}\n")

# --- Divisão em parágrafos e tokenização ---
print("# Divisão em parágrafos e tokenização")
paragrafos = preparar_paragrafos(texto)
tokenizados = tokenizar_paragrafos(paragrafos)

if paragrafos:
    print(f"Primeiro parágrafo:\n{paragrafos[0]}\n")
    print(f"Primeiro parágrafo tokenizado:\n{tokenizados[0]}\n")
else:
    print("Nenhum parágrafo foi extraído. Verifique a função preparar_paragrafos.")

# --- Indexação com BM25 ---
print("# Indexação com BM25 e Exemplo de Consulta")
bm25 = configurar_bm25(tokenizados)

consulta = "Qual a base legal do curso de Licenciatura em Computação?"
# IMPORTANTE: Tokenize a consulta da mesma forma que os documentos foram tokenizados.
consulta_tokenizada = word_tokenize(consulta.lower())

top_docs = bm25.get_top_n(consulta_tokenizada, paragrafos, n=3)

print(f"\nConsulta: \"{consulta}\"")
print(f"Documentos recuperados pelo BM25 (Top 3):\n")
for i, doc in enumerate(top_docs):
    print(f"--- Documento {i+1} ---\n{doc}\n")

# --- Testando a parte de geração (Opcional - requer API Key) ---
# Se você quiser testar a API Gemini/OpenAI aqui no Colab, pode adicionar:
# from app import carregar_modelo, responder
#
# print("\n# Testando a Geração de Respostas com o LLM (requer API Key configurada)")
# try:
#     modelo, tokenizer = carregar_modelo() # Retorna o modelo da API e o tokenizer para estimativa
#     resposta_llm = responder(consulta, bm25, paragrafos, (modelo, tokenizer))
#     print(f"Resposta do Chatbot:\n{resposta_llm}\n")
# except Exception as e:
#     print(f"Não foi possível testar a geração de resposta (erro: {e}). Certifique-se de que sua API Key está configurada no .env.")